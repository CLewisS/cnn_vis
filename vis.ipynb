{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440bc3f7-3a3c-4f43-865c-f0eed6744882",
   "metadata": {},
   "source": [
    "# Feature Visualization Learning Journey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb485b0-41ae-4c89-9a3f-8907222955f9",
   "metadata": {},
   "source": [
    "After reading [Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/) I was quite interested in how some of the visuals were generated! It seemed like the authors were \"asking\" the model what it was looking for, and I wanted to know how I could have a similar \"conversation\" with a model! With this motivation, I started trying to generate visualizations similar to those found in the paper, however, I don't have much experience with PyTorch or interacting with neural networks, so I encountered some issues while doing so, and I figured a brief description of the \"journey\" I took could help others starting from a similar place as I get started on conversing with models too!\n",
    "\n",
    "Note: I won't cover neural network basics, so it may be best to learn the basics before following the steps outlined here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72b10d-ed58-4b54-bab5-f941556eb57e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Boring Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554b045-53a2-4126-b2d3-e7627b461156",
   "metadata": {},
   "source": [
    "Just some boring imports to get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2fd07-1df8-43b2-ba25-d7b884f2ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0c2ec-2f71-4c3c-931e-f0808980beba",
   "metadata": {},
   "source": [
    "And here are some basic parameters used throughout the notebook. Also pretty boring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa768e4a-149c-4485-b899-0b395d480c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "num_channels = 3\n",
    "num_itrs = 512\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889f9dd-58bd-45cd-a5b5-43ad1ffc8f04",
   "metadata": {},
   "source": [
    "The feature visualizations all start out as random images which we will generated like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074675cb-0693-47be-b504-209c3c5b7b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(1, num_channels, img_size, img_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff56137-f4e5-409a-ba5e-16e705ebe7a5",
   "metadata": {},
   "source": [
    "Now, to view images we'll use the below utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c714b9-57f2-482f-8006-146b077ffb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    img = img.detach().to(device).squeeze(dim=0).permute(1, 2, 0)\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c7566-3711-4635-bb81-dad2939f90ba",
   "metadata": {},
   "source": [
    "Now let's visualize the image we generated above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81369859-04ec-47b8-8c0e-7b7f02915de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3399ad86-5f2d-47c6-80e4-f68863a8e69d",
   "metadata": {},
   "source": [
    "Okay sweet! Now that we can generate and view images, let's get to generating feature visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da140f4a-1939-4fe5-9621-3c5b1f5ef305",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Here's the first interesting bit! Here we're loading the model which we'll use throughout the notebook. The model we'll use is [ResNet18](https://pytorch.org/hub/pytorch_vision_resnet/) ([original paper](https://arxiv.org/abs/1512.03385)) which is pretrained on ImageNet. The techniques should apply to any pretrained image model, but we will use ResNet18 because it's a small model, and thus runs reasonably quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ccc646-f33c-410c-b2a1-5a51c0fce920",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a3baf-6d5f-4032-8c24-0fd5ccaf9f6a",
   "metadata": {},
   "source": [
    "## Basic Feature Visualization\n",
    "\n",
    "To visualize features we'll iteratively \"tweek\" (adjust based on the gradients) the image so it activates a part of the network more. To do this we'll perform the following steps for each iteration:\n",
    "1. Perform a forward pass of the model with the current image\n",
    "2. Compute the mean activation of the channel of interest (this acts as our loss function)\n",
    "3. Calculate the gradients\n",
    "4. Update the image using the gradients of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982811e-14c9-4f0e-ad9b-278f91e22c0b",
   "metadata": {},
   "source": [
    "The first thing we need to perform the above steps is a way to get the output of the layer we wish to visualize. We will store the outputs in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f49f79-d10b-4b50-8ba6-cb246973ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf09ef-9912-424b-b257-9b9d50a8f4a3",
   "metadata": {},
   "source": [
    "And we will use a hook to capture the outputs (we'll [register the hook](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook) to the forward pass of the layer of interest later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba24785-2267-4a3d-9770-1144be3d98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_hook(layer_name):\n",
    "    def hook(module, input, output):\n",
    "        activations[layer_name] = output\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a8d64-8cd1-4e31-8d5a-2d66a2f289e0",
   "metadata": {},
   "source": [
    "Now that we have a way to capture the outputs, we need a way to compute the mean activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e759df-7c51-40aa-92b2-899059fa0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_fcn(layer, channel_idx):\n",
    "    def loss_fcn():\n",
    "        if (activations[layer].dim() == 2):\n",
    "            layer_activations = -activations[layer][:, channel_idx]\n",
    "        else:\n",
    "            layer_activations = -activations[layer][:, channel_idx, :, :]\n",
    "        return layer_activations.mean()\n",
    "    return loss_fcn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42422d7c-6fc6-4271-8123-b52774a21dca",
   "metadata": {},
   "source": [
    "With the loss function above, we can write a function for a single optimization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781dd86-a9dd-4204-b7b2-f25c1d2adfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_step(model, img, optimizer, loss_fcn):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1. Perform a forward pass of the model with the current image\n",
    "    model(img)\n",
    "\n",
    "    # 2. Compute the mean activation of the channel of interest (this acts as our loss function)\n",
    "    loss = loss_fcn()\n",
    "    # 3. Calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. Update the image using the gradients of the loss function\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a040b3-ce7b-4ca6-9a5c-de1d599529b7",
   "metadata": {},
   "source": [
    "Let's put it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc40055-cad8-4423-854c-461bfd2ea920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_img(model, img, layer_name, channel_idx):\n",
    "    # Register the hook for capturing the layer output\n",
    "    layer = getattr(model, layer_name)\n",
    "    handle = layer.register_forward_hook(get_layer_hook(layer_name))\n",
    "\n",
    "    model.eval()\n",
    "    # Run a forward pass of the model to populate the activations dictionary.\n",
    "    # This is required so we can enable the gradients on the layer output.\n",
    "    pred = model(img)\n",
    "    \n",
    "    # I initially didn't include this, and it took me a little while to figure out why\n",
    "    # my images were not updating XD.\n",
    "    activations[layer_name] = activations[layer_name].requires_grad_().to(device)\n",
    "\n",
    "    # We'll use the Adam optimizer\n",
    "    optimizer = torch.optim.Adam([img], lr=0.05)\n",
    "\n",
    "    # Get loss function for the layer and channel of interest\n",
    "    loss_fcn = get_loss_fcn(layer_name, channel_idx)\n",
    "\n",
    "    # Iteratively tweek the image\n",
    "    for i in range(num_itrs):\n",
    "        opt_step(model, img, optimizer, loss_fcn)\n",
    "\n",
    "    handle.remove()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be73bbc-a72f-4fa6-89b5-632329d63751",
   "metadata": {},
   "source": [
    "Now that we have our basic feature visualization generator completed we can create our first visualization! We'll start by visualizing the output of one of the neurons in the last layer of the model because these neurons are often the easiest to interpret (each neuron corresponds to one of the output classes). First we need to find the name of the last layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b42c3-4172-401e-9fcf-63df1b53cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(map(lambda x: x[0], model.named_children())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48226175-a734-4544-8683-d1177d2a3420",
   "metadata": {},
   "source": [
    "Okay, it's \"fc\". Let's generate an image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc3f20-6280-4afb-be1f-7129092a61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(1, num_channels, img_size, img_size, requires_grad=True).to(device)\n",
    "out_img = opt_img(model, img, \"fc\", 9)\n",
    "show_img(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65231d-9bb6-4dac-84df-7d1a08e71e6d",
   "metadata": {},
   "source": [
    "This is probably not what you were hoping for. I found myself somewhat stuck at this point! The visualizations seemed to have some order to them, but they still seemed like mostly noise. After a little while of trying to figure out why the visualizations weren't nearly as interesting as visualizations I had seen others generate, I found a particularly useful section in a [feature visualization paper](https://distill.pub/2017/feature-visualization/): [The Enemy of Feature Visualization](https://distill.pub/2017/feature-visualization/#enemy-of-feature-vis). The technique from the paper I tried was slightly transforming the image before each optimization step, so let's see how that impacts the visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517be80-2be8-49da-9c25-0e5fbec2cff9",
   "metadata": {},
   "source": [
    "## Transformation Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceafd88-4b6a-49ac-a6ca-7545ce8030f4",
   "metadata": {},
   "source": [
    "In order to transform the image before the optimization step we need to modify our `opt_img` function a bit! So let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e260c-db90-43b7-a438-ce0d5b55eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_img(model, img, layer_name, channel_idx, transforms=None):\n",
    "    layer = getattr(model, layer_name)\n",
    "    handle = layer.register_forward_hook(get_layer_hook(layer_name))\n",
    "\n",
    "    model.eval()\n",
    "    pred = model(img)\n",
    "    activations[layer_name] = activations[layer_name].requires_grad_().to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam([img], lr=0.05)\n",
    "\n",
    "    loss_fcn = get_loss_fcn(layer_name, channel_idx)\n",
    "\n",
    "    for i in range(num_itrs):\n",
    "        # Here's the new bit!\n",
    "        if transforms is not None:\n",
    "            tform_img = transforms(img)\n",
    "        else:\n",
    "            tform_img = img\n",
    "        opt_step(model, tform_img, optimizer, loss_fcn)\n",
    "\n",
    "    handle.remove()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6f867-0fc3-4f92-8f89-9c554b6fdb8f",
   "metadata": {},
   "source": [
    "The paper includes three different transformations: jittering, rotation, and scaling. Let's use all three!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db47a6f-79a4-4552-b105-c1dee345265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomCrop(img_size, padding=random.randint(0, 8)), # jitter\n",
    "    torchvision.transforms.RandomRotation((-45, 45)), # rotate\n",
    "    torchvision.transforms.RandomResizedCrop(img_size, scale=(0.9, 1.2), ratio=(1.0, 1.0)) # scale\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2285e3b-97d8-4258-af3f-c6830bd6d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(1, num_channels, img_size, img_size, requires_grad=True).to(device)\n",
    "out_img = opt_img(model, img, \"fc\", 9, transforms)\n",
    "show_img(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40606ef-974c-4a2c-a505-aa96d96700c8",
   "metadata": {},
   "source": [
    "This seems much more interesting! Can you figure out what it is?\n",
    "\n",
    "It's an ostrich! It doesn't look *exactly* like an ostrich, but you can at least see that it has some ostrich features: you can kind of make out the long legs; the dark, round body; and the long neck!\n",
    "\n",
    "This visualization is much more interesting than the one before! Now that we can generate visualizations like this, let's explore some of the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e259a-42e3-4ebf-b03c-78040a91a1ba",
   "metadata": {},
   "source": [
    "## Hidden Layer Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f91c1c-1a4f-4085-8e9d-76848b9fca0d",
   "metadata": {},
   "source": [
    "Let's start out by generating visualizations without transformations, so we can see the impact of transformations on hidden layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dce7995-3512-4a8b-b263-6ef3bfcec9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(1, num_channels, img_size, img_size, requires_grad=True).to(device)\n",
    "out_img = opt_img(model, img, \"layer2\", 71)\n",
    "show_img(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccabe96-fda2-4634-81d1-6bd071d2be6c",
   "metadata": {},
   "source": [
    "There's definitely some order to this image, but it still seems like there's a lot of noise, so let's now try with transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9e3a82-8290-48e9-8718-5c873bad074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(1, num_channels, img_size, img_size, requires_grad=True).to(device)\n",
    "out_img = opt_img(model, img, \"layer2\", 71, transforms)\n",
    "show_img(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15806698-9851-4aa8-a013-0e0273ed922f",
   "metadata": {},
   "source": [
    "Again, it's much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee42cf-f636-445b-953f-4fe2e8480377",
   "metadata": {},
   "source": [
    "## Where can this go?\n",
    "\n",
    "This is great! We were able to generate some feature visualizations! With the above code we can generate visualizations for different parts of the model, and begin to \"ask\" the model what it's looking for so we can interpret what the model knows, and how it makes it's decisions. To further improve the quality of the images there are a number of other techniques in the [Feature Visualizations](https://distill.pub/2017/feature-visualization) paper that can be applied. In addition to improving the quality of the visualizations, they can be used together with other techniques to improve our understanding of neural networks. Examples of this can be seen in [this](https://distill.pub/2018/building-blocks/) paper, and in other papers. The feature visualizations we generated are just the start of interpreting image models! While generating the feature visualizations is interesting, what can be done with the visualizations is even more so!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
